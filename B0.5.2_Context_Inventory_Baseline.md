# B0.5.2 Context Inventory Baseline

**Generated**: 2025-12-13
**Methodology**: Forensic Evidence Collection
**Scope**: B0.5.2 Pre-Remediation Reality Map
**Purpose**: Establish empirical baseline for hypothesis-driven B0.5.2 completion

---

## Document Status

This is a **forensic inventory only**. No remediations have been applied. All findings are evidence-backed with file paths, line numbers, migration IDs, and configuration pointers.

---

## 1. Context Anchor: B0.1‚ÄìB0.4 + B0.5.1 Reality Map

### B0.1: Contract Generation Infrastructure

**Status**: ‚úÖ Exists
**Location**: [contracts/](contracts/)
**Evidence**:
- OpenAPI 3.1.0 specifications in domain-organized structure
- Contracts for: auth, attribution, reconciliation, export, health, webhooks (shopify, stripe, paypal, woocommerce)
- Common components: [contracts/_common/v1/](contracts/_common/v1/) (components.yaml, pagination.yaml, parameters.yaml)
- Baseline versioning: [contracts/*/baselines/v1.0.0/](contracts/)
- CI validation: [.github/workflows/ci.yml:21-61](.github/workflows/ci.yml#L21-L61) (validate-contracts job)

**Integration Point for B0.5.2**: None identified. Worker tasks currently don't reference contracts.

---

### B0.2: Mock Server Orchestration

**Status**: ‚úÖ Exists (Prism-based)
**Location**: Referenced in contracts and CI
**Evidence**:
- Mock server technology: Prism CLI (Stoplight)
- Documentation: [contracts/README.md:89-97](contracts/README.md#L89-L97)
  - Auth: http://localhost:4010
  - Attribution: http://localhost:4011
  - Reconciliation: http://localhost:4012
  - Export: http://localhost:4013
  - Health: http://localhost:4014
  - Webhooks: http://localhost:4015-4018
- CI usage: [.github/workflows/ci.yml:220-234](.github/workflows/ci.yml#L220-L234) (test-integration job starts Prism mocks)

**Integration Point for B0.5.2**: None identified. Worker infrastructure not represented in mock server orchestration.

---

### B0.3: RLS + app_user + Tenant GUC

**Status**: ‚úÖ Exists and Operational
**Location**: Migrations, session management
**Evidence**:

#### Migrations
- RLS policies: [alembic/versions/001_core_schema/202511131120_add_rls_policies.py](alembic/versions/001_core_schema/202511131120_add_rls_policies.py)
- Role grants: [alembic/versions/001_core_schema/202511131121_add_grants.py](alembic/versions/001_core_schema/202511131121_add_grants.py)
- Tenant GUC: `app.current_tenant_id` session variable enforces RLS filters

#### Session Helper
- [app/db/session.py:119-131](app/db/session.py#L119-L131) `set_tenant_guc(session, tenant_id, local=True)`
- [app/db/session.py:78-102](app/db/session.py#L78-L102) `get_session(tenant_id)` context manager sets GUC before yielding

#### Role
- `app_user`: Restricted role with RLS enforcement
- CI provisioning: [.github/workflows/ci.yml:135-143](.github/workflows/ci.yml#L135-L143) creates app_user, app_rw, app_ro

#### B0.5.1 Integration
- Celery tasks use same engine: [app/tasks/context.py:17](app/tasks/context.py#L17) imports `from app.db.session import engine, set_tenant_guc`
- `@tenant_task` decorator: [app/tasks/context.py:36-75](app/tasks/context.py#L36-L75) enforces tenant_id, sets GUC via `_set_tenant_guc_global`

**Integration Point for B0.5.2**: Worker DLQ (if implemented) must respect tenant_id and set GUC for queries under app_user role.

---

### B0.4: Observability Patterns (Ingestion Service)

**Status**: ‚úÖ Exists
**Location**: app/observability/, app/ingestion/
**Evidence**:

#### Metrics (Prometheus)
- [app/observability/metrics.py](app/observability/metrics.py): Defines Counter/Histogram for ingestion events
  - `events_ingested_total` (lines 5-9)
  - `events_duplicate_total` (lines 11-15)
  - `events_dlq_total` (lines 17-21)
  - `ingestion_duration_seconds` (lines 23-28)
- Labels: `["tenant_id", "vendor", "event_type", "error_type"]`

#### Structured Logging
- [app/observability/logging_config.py](app/observability/logging_config.py): JSON formatter with correlation_id + tenant_id context
- [app/observability/context.py](app/observability/context.py): Context vars for request/business correlation, tenant_id (inferred from imports)

#### DLQ Pattern
- [app/ingestion/dlq_handler.py](app/ingestion/dlq_handler.py): DLQHandler class with:
  - Error classification (transient vs permanent): lines 87-130
  - Exponential backoff retry: lines 257-367
  - `dead_events` table routing: lines 167-255
  - Remediation status state machine: lines 47-84

#### FastAPI Metrics Endpoint
- [app/main.py](app/main.py): FastAPI app
- Metrics exposure: Not found in main.py (likely in separate route or middleware)
- B0.4 doc reference: [backend/docs/api/B0.4_INGESTION_SERVICE.md:31-35](backend/docs/api/B0.4_INGESTION_SERVICE.md#L31-L35) specifies `/metrics` GET endpoint

**Integration Point for B0.5.2**: Worker observability must follow same patterns (Prometheus metrics, JSON logs, correlation IDs) and use consistent metric/label naming.

---

### B0.5.1: Celery Foundation

**Status**: ‚úÖ Empirically Complete (per B0.5.1_EXECUTION_SUMMARY.md)
**Location**: app/celery_app.py, app/tasks/, app/observability/worker_monitoring.py
**Evidence**:

#### Celery App Configuration
- [app/celery_app.py:60-79](app/celery_app.py#L60-L79)
  - Broker: `sqla+postgresql://...` (Postgres SQLAlchemy transport)
  - Result backend: `db+postgresql://...` (Postgres database backend)
  - Task serializer: JSON
  - Include list: `["app.tasks.housekeeping", "app.tasks.maintenance", "app.tasks.llm"]`

#### Broker/Backend DSN Construction
- [app/celery_app.py:38-57](app/celery_app.py#L38-L57)
  - `_build_broker_url()`: Returns `sqla+{sync_dsn}` (defaults to DATABASE_URL if CELERY_BROKER_URL unset)
  - `_build_result_backend()`: Returns `db+{sync_dsn}` (defaults to DATABASE_URL if CELERY_RESULT_BACKEND unset)
  - Sync DSN derivation: [app/celery_app.py:22-35](app/celery_app.py#L22-L35) strips asyncpg driver, removes channel_binding param

#### Worker Monitoring Server
- [app/observability/worker_monitoring.py](app/observability/worker_monitoring.py)
  - HTTP server: WSGI on port CELERY_METRICS_PORT (default 9540)
  - Routes:
    - `/metrics`: Prometheus text format (line 60-61)
    - `/health`: JSON with broker + DB checks (lines 63-77)
  - Started via signal: [app/celery_app.py:82-96](app/celery_app.py#L82-L96) `worker_process_init` signal calls `start_worker_http_server`

#### Metrics Instrumentation
- [app/observability/metrics.py:30-53](app/observability/metrics.py#L30-L53): Celery-specific metrics
  - `celery_task_started_total`
  - `celery_task_success_total`
  - `celery_task_failure_total`
  - `celery_task_duration_seconds`
- Signal hooks: [app/celery_app.py:99-129](app/celery_app.py#L99-L129)
  - `task_prerun`: Increments started counter, records start time
  - `task_postrun`: Observes duration, increments success counter
  - `task_failure`: Increments failure counter, logs error

#### Configuration Settings
- [app/core/config.py:43-53](app/core/config.py#L43-L53)
  - `CELERY_BROKER_URL: Optional[str]` (line 44-47)
  - `CELERY_RESULT_BACKEND: Optional[str]` (line 48-51)
  - `CELERY_METRICS_PORT: int = 9540` (line 52)
  - `CELERY_METRICS_ADDR: str = "0.0.0.0"` (line 53)

#### Migrations
- [alembic/versions/006_celery_foundation/202512120900_celery_tables.py](alembic/versions/006_celery_foundation/202512120900_celery_tables.py)
  - Creates: kombu_queue, kombu_message, celery_taskmeta, celery_tasksetmeta
  - Grants: Full CRUD to app_user (lines 62-76)
  - Branch label: `celery_foundation` (line 9)

#### CI Integration
- [.github/workflows/ci.yml:88-177](.github/workflows/ci.yml#L88-L177) `celery-foundation` job
  - Postgres service provisioned
  - Migration: `alembic upgrade celery_foundation@head` (line 151)
  - Worker start: `celery -A app.celery_app.celery_app worker -P solo -c 1 --loglevel=INFO` (line 158)
  - Test execution: `pytest tests/test_b051_celery_foundation.py -q` (line 169)

**Integration Point for B0.5.2**: All B0.5.2 enhancements must build on this foundation without breaking existing broker/backend/monitoring server configuration.

---

## 2. B0.5.2 Completion Criteria (Verbatim)

Per directive, B0.5.2 is complete when:

1. **Fixed queue topology + deterministic task registration**: Named queues, routing keys, worker bindings, and stable task names with explicit discovery.

2. **Worker-level DLQ schema**: Postgres tables + routing convention for failed background jobs, ensuring deterministic failure capture and replay.

3. **Empirically closed worker observability operability**:
   - `curl -sf http://127.0.0.1:9546/health` returns HTTP 200 with truthful broker + DB readiness
   - `curl -sf http://127.0.0.1:9546/metrics` returns HTTP 200 with expected Celery metric families
   - Evidence captured in CI logs or artifact bundle

4. **Minimal domain logic**: Only infrastructure-verification tasks (ping/no-op/fail-fast) for routing, DLQ, and observability validation.

5. **CI infrastructure soundness**: Postgres-only provisioning, migrations, worker start, B0.5.2 test suite execution. Failures acceptable only if out-of-scope; infrastructure failures are blockers.

---

## 3. Findings by Criterion

### Criterion 1: Queue Topology + Task Registration

#### 3.1.1 Queue Topology

**Status**: ‚ùå **MISSING**

**Current State**:
- No explicit queue definitions found
- No `task_routes` configuration in [app/celery_app.py](app/celery_app.py)
- No `task_queues` configuration in [app/celery_app.py](app/celery_app.py)
- No `Queue` imports from `kombu` or `celery`

**Evidence**:
```bash
# Grep results for queue-related config
$ grep -rn "task_routes\|task_queues\|Queue" backend/app/
# No results
```

**Expected for B0.5.2**:
- Explicit queue declarations (e.g., `default`, `priority`, `maintenance`)
- Routing rules mapping task names to queue names
- Worker binding specification (CLI flags or config)

**Gap**: Queue topology is implicit (default queue only). No fixed, documented topology.

---

#### 3.1.2 Task Registration

**Status**: üü° **PARTIAL** (Explicit includes, but no autodiscovery strategy documented)

**Current State**:
- Task registration via explicit `include=` list: [app/celery_app.py:74-78](app/celery_app.py#L74-L78)
  ```python
  include=[
      "app.tasks.housekeeping",
      "app.tasks.maintenance",
      "app.tasks.llm",
  ],
  ```
- Additional explicit imports at module bottom: [app/celery_app.py:135-137](app/celery_app.py#L135-L137)
  ```python
  import app.tasks.housekeeping  # noqa: E402,F401
  import app.tasks.maintenance  # noqa: E402,F401
  import app.tasks.llm  # noqa: E402,F401
  ```

**Registered Tasks** (from test evidence [tests/test_b051_celery_foundation.py:168-176](tests/test_b051_celery_foundation.py#L168-L176)):
- `app.tasks.housekeeping.ping`
- `app.tasks.maintenance.refresh_all_materialized_views`
- `app.tasks.llm.route`
- `app.tasks.llm.explanation`
- `app.tasks.llm.investigation`
- `app.tasks.llm.budget_optimization`

**Task Name Stability**:
- ‚úÖ Task names are explicit: `@celery_app.task(bind=True, name="app.tasks.housekeeping.ping")`
- ‚úÖ Naming convention: `app.tasks.{module}.{function_name}`

**Gap**: No documented policy on when to use `include=` vs `autodiscover_tasks`. No enforcement of task name stability in tests.

---

### Criterion 2: Worker-Level DLQ Schema

**Status**: ‚ùå **MISSING**

**Current State**:
- **Ingestion DLQ exists**: [app/ingestion/dlq_handler.py](app/ingestion/dlq_handler.py) routes failed webhook ingestion events to `dead_events` table
- **Migration**: [alembic/versions/003_data_governance/202511151440_add_dead_events_retry_tracking.py](alembic/versions/003_data_governance/202511151440_add_dead_events_retry_tracking.py) creates `dead_events` table with:
  - `event_type`, `error_type`, `error_message`, `error_traceback`
  - `retry_count`, `last_retry_at`
  - `remediation_status` (pending, in_progress, resolved, ignored)
  - `resolved_at`, `remediation_notes`
- **RLS**: `dead_events` table has tenant_id and RLS policy (inferred from B0.3 pattern)

**Worker-Level DLQ**:
- ‚ùå No separate table for Celery task failures
- ‚ùå No signal hook capturing task failures to DLQ
- ‚ùå Current `task_failure` signal only logs + increments counter: [app/celery_app.py:115-129](app/celery_app.py#L115-L129)

**Expected for B0.5.2**:
- New table: `celery_task_failures` or similar (Postgres schema)
- Columns: task_id, task_name, args, kwargs, error_type, error_message, traceback, retry_count, tenant_id (optional), correlation_id
- Signal hook: Capture `task_failure` and write to DLQ table
- Retry mechanism: Resubmit task from DLQ or mark as abandoned

**Gap**: Worker failures are logged and metered but not persisted for replay. No DLQ table or routing convention for background task failures.

---

### Criterion 3: Observability Operability (Monitoring Server Endpoints)

**Status**: üü° **INFRASTRUCTURE COMPLETE, EVIDENCE PENDING**

#### 3.3.1 Monitoring Server Implementation

**Status**: ‚úÖ **EXISTS**

**Location**: [app/observability/worker_monitoring.py](app/observability/worker_monitoring.py)

**Port Binding**:
- Config: [app/core/config.py:52-53](app/core/config.py#L52-L53)
  - `CELERY_METRICS_PORT: int = 9540` (default)
  - `CELERY_METRICS_ADDR: str = "0.0.0.0"`
- CI override: [.github/workflows/ci.yml:110-111](.github/workflows/ci.yml#L110-L111) sets port to `9546`
- Test override: [tests/test_b051_celery_foundation.py:23](tests/test_b051_celery_foundation.py#L23) defaults to `9546`

**Routes**:

1. `/metrics` (Prometheus)
   - Implementation: [worker_monitoring.py:56-61](app/observability/worker_monitoring.py#L56-L61)
   - Returns: `prometheus_client.make_wsgi_app()` output (text/plain exposition format)

2. `/health` (JSON)
   - Implementation: [worker_monitoring.py:63-77](app/observability/worker_monitoring.py#L63-L77)
   - Checks:
     - Broker connectivity: [worker_monitoring.py:24-34](app/observability/worker_monitoring.py#L24-L34) `_check_broker(celery_app)`
     - Database connectivity: [worker_monitoring.py:37-49](app/observability/worker_monitoring.py#L37-L49) `_check_database()` (calls `validate_database_connection()`)
   - Response schema:
     ```json
     {
       "status": "ok" | "unhealthy",
       "broker": "ok" | "error",
       "database": "ok" | "error"
     }
     ```
   - HTTP status: 200 if healthy, 503 if unhealthy

**Startup**:
- Signal hook: [app/celery_app.py:82-96](app/celery_app.py#L82-L96) `worker_process_init` calls `start_worker_http_server`
- Thread: Daemon thread started in background (lines 111-112 of worker_monitoring.py)

---

#### 3.3.2 Metrics Definitions

**Status**: ‚úÖ **DEFINED AND WIRED**

**Metrics** ([app/observability/metrics.py:30-53](app/observability/metrics.py#L30-L53)):
- `celery_task_started_total{task_name}` (Counter)
- `celery_task_success_total{task_name}` (Counter)
- `celery_task_failure_total{task_name}` (Counter)
- `celery_task_duration_seconds{task_name}` (Histogram, buckets: 0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 5, 10)

**Signal Hooks** ([app/celery_app.py:99-129](app/celery_app.py#L99-L129)):
- `task_prerun`: Increments `started_total`, records start time (line 100-102)
- `task_postrun`: Observes `duration_seconds`, increments `success_total` if SUCCESS (lines 105-112)
- `task_failure`: Increments `failure_total`, logs error (lines 115-129)

**Verification**:
- Test: [tests/test_b051_celery_foundation.py:116-125](tests/test_b051_celery_foundation.py#L116-L125) curls `/metrics` and asserts presence of `celery_task_success_total`

---

#### 3.3.3 CI Validation of Endpoints

**Status**: ‚ùå **NOT VALIDATED IN CI**

**Current CI** ([.github/workflows/ci.yml:88-177](.github/workflows/ci.yml#L88-L177)):
- Provisions Postgres ‚úÖ
- Creates roles ‚úÖ
- Runs migrations ‚úÖ
- Starts worker ‚úÖ
- Runs tests ‚úÖ
- **Does NOT curl /metrics or /health endpoints directly in CI logs**

**Test Coverage** ([tests/test_b051_celery_foundation.py](tests/test_b051_celery_foundation.py)):
- Lines 116-125: Test curls `/metrics` from worker HTTP server (port 9546), asserts `celery_task_success_total` present
- Lines 121-125: Test curls `/health`, asserts `broker: ok` and `database: ok`

**Evidence Source**:
- Test executes curl but CI **does not capture curl output as artifact**
- Validation is indirect (test passes ‚Üí endpoints work) rather than direct (CI logs show `curl -sf http://127.0.0.1:9546/metrics`)

**Gap**: B0.5.2 criterion requires "evidence captured in CI logs or CI artifact bundle". Current CI does not produce curl output artifacts.

**Expected for B0.5.2**:
- Add CI step: `curl -sf http://127.0.0.1:9546/health > evidence/worker_health.json`
- Add CI step: `curl -sf http://127.0.0.1:9546/metrics > evidence/worker_metrics.txt`
- Upload evidence/ directory as artifact

---

#### 3.3.4 Conflict: FastAPI /metrics vs Worker /metrics

**Status**: ‚ö†Ô∏è **POTENTIAL CONFUSION** (different surfaces)

**FastAPI /metrics**:
- Test: [tests/test_b051_celery_foundation.py:129-147](tests/test_b051_celery_foundation.py#L129-L147) tests FastAPI `/metrics` endpoint
- Uses AsyncClient to query FastAPI app (not worker HTTP server)
- Expected to expose same Prometheus registry (shared global)

**Worker /metrics**:
- Test: [tests/test_b051_celery_foundation.py:116-119](tests/test_b051_celery_foundation.py#L116-L119) curls worker HTTP server directly

**Implication for B0.5.2**:
- Criterion specifies `http://127.0.0.1:9546/metrics` (worker server, not FastAPI)
- CI must target **worker server**, not FastAPI `/metrics` route

---

### Criterion 4: Minimal Domain Logic

**Status**: ‚úÖ **SATISFIED**

**Current Tasks**:

1. **Housekeeping** ([app/tasks/housekeeping.py](app/tasks/housekeeping.py)):
   - `ping(fail, tenant_id)`: No-op task, returns `{"status": "ok"}`, optionally sets tenant GUC

2. **Maintenance** ([app/tasks/maintenance.py](app/tasks/maintenance.py)):
   - `refresh_all_materialized_views_task()`: Refreshes matviews (global, non-tenant)
   - `scan_for_pii_contamination_task(tenant_id)`: Stub PII scan, validates DB connection + tenant GUC
   - `enforce_data_retention_task(tenant_id)`: Stub retention enforcement

3. **LLM** ([app/tasks/llm.py](app/tasks/llm.py)):
   - `llm_routing_worker(payload, tenant_id)`: Stub routing task, returns `{"status": "accepted", "route": "noop"}`
   - `llm_explanation_worker(payload, tenant_id)`: Stub explanation task
   - `llm_investigation_worker(payload, tenant_id)`: Stub investigation task
   - `llm_budget_optimization_worker(payload, tenant_id)`: Stub budget task

**Assessment**:
- ‚úÖ All tasks are stubs or infrastructure-verification tasks
- ‚úÖ No attribution processing logic
- ‚úÖ No matview refresh business logic beyond SQL REFRESH MATERIALIZED VIEW call
- ‚úÖ Tasks exercise tenant context, RLS, metrics, logging

**No gaps**: Criterion satisfied.

---

### Criterion 5: CI Infrastructure Soundness

**Status**: üü° **MOSTLY SOUND** (migrations + worker start working, evidence capture gaps)

#### 5.5.1 Service Provisioning

**Status**: ‚úÖ **Postgres-only**

**Evidence** ([.github/workflows/ci.yml:92-105](.github/workflows/ci.yml#L92-L105)):
- Service: `postgres:15-alpine`
- Env: POSTGRES_USER, POSTGRES_PASSWORD, POSTGRES_DB
- Health check: `pg_isready`
- Port: 5432

**No Redis, RabbitMQ, or Kafka**: ‚úÖ Postgres-only stack confirmed.

---

#### 5.5.2 Role Provisioning

**Status**: ‚úÖ **COMPLETE**

**Evidence** ([.github/workflows/ci.yml:132-143](.github/workflows/ci.yml#L132-L143)):
```sql
CREATE USER app_user WITH PASSWORD 'app_user';
CREATE USER app_rw WITH PASSWORD 'app_rw';
CREATE USER app_ro WITH PASSWORD 'app_ro';
CREATE DATABASE skeldir_validation OWNER app_user;
GRANT ALL PRIVILEGES ON DATABASE skeldir_validation TO app_user;
GRANT ALL ON SCHEMA public TO app_user;
GRANT ALL ON SCHEMA public TO app_rw;
GRANT USAGE ON SCHEMA public TO app_ro;
```

**Alignment with B0.3**: ‚úÖ Matches RLS role expectations.

---

#### 5.5.3 Migration Execution

**Status**: ‚úÖ **WORKING** (targeted celery_foundation branch)

**Command** ([.github/workflows/ci.yml:151](.github/workflows/ci.yml#L151)):
```bash
alembic upgrade celery_foundation@head
```

**Strategy**:
- Targets specific migration branch (not full schema)
- Avoids custom GUCs required by full schema (out of B0.5.1 scope per H6.10 in B0.5.1_EXECUTION_SUMMARY.md)

**Migration Applied**:
- [alembic/versions/006_celery_foundation/202512120900_celery_tables.py](alembic/versions/006_celery_foundation/202512120900_celery_tables.py)
- Creates: kombu_queue, kombu_message, celery_taskmeta, celery_tasksetmeta
- Grants CRUD to app_user

**Validation** (from B0.5.1 execution summary):
- ‚úÖ Migration applies cleanly in CI (commit 772ad79)

---

#### 5.5.4 Worker Startup

**Status**: ‚úÖ **WORKING**

**Command** ([.github/workflows/ci.yml:158](.github/workflows/ci.yml#L158)):
```bash
celery -A app.celery_app.celery_app worker -P solo -c 1 --loglevel=INFO &
```

**Pool**: `solo` (single-threaded, deterministic for CI)
**Concurrency**: 1
**Background**: Runs in background, PID captured to `/tmp/celery.pid`
**Wait**: 10-second sleep after start (line 161)

**Validation** (from B0.5.1):
- ‚úÖ Worker starts successfully (commit 772ad79)
- ‚úÖ Monitoring server starts (inferred from test passing)

---

#### 5.5.5 Test Execution

**Status**: ‚úÖ **WORKING**

**Command** ([.github/workflows/ci.yml:169](.github/workflows/ci.yml#L169)):
```bash
pytest tests/test_b051_celery_foundation.py -q
```

**Tests** ([tests/test_b051_celery_foundation.py](tests/test_b051_celery_foundation.py)):
1. `test_celery_config_uses_postgres_sqla`: Validates broker/backend DSN format
2. `test_ping_task_runs_and_persists_result`: Validates task execution + result persistence + metrics/health endpoints
3. `test_metrics_exposed_via_fastapi`: Validates FastAPI `/metrics` endpoint
4. `test_worker_logs_are_structured`: Validates JSON log format
5. `test_registered_tasks_include_stubs`: Validates task registration
6. `test_tenant_task_enforces_and_sets_guc`: Validates @tenant_task decorator + GUC setting

**Validation** (from B0.5.1):
- ‚úÖ Tests passing (inferred from "empirically complete" status)

---

#### 5.5.6 Evidence Capture

**Status**: ‚ùå **GAP**

**Current State**:
- Tests execute curl but output not captured as CI artifact
- No dedicated CI steps for curl evidence collection

**Expected for B0.5.2**:
- CI step: curl health endpoint, save to artifact
- CI step: curl metrics endpoint, save to artifact
- Upload artifact bundle

---

## 4. Gap Matrix

| Criterion | Component | Status | Evidence Pointer | Gap Description |
|-----------|-----------|--------|------------------|-----------------|
| **#1: Queue Topology** | Queue definitions | ‚ùå Missing | N/A | No explicit queue declarations (task_routes, task_queues) |
| **#1: Queue Topology** | Routing rules | ‚ùå Missing | N/A | No task routing configuration |
| **#1: Queue Topology** | Worker bindings | ‚ùå Missing | N/A | No documented worker queue binding strategy |
| **#1: Task Registration** | Discovery mechanism | üü° Partial | [celery_app.py:74-78](app/celery_app.py#L74-L78) | Explicit `include=` exists but no autodiscovery policy documented |
| **#1: Task Registration** | Name stability tests | ‚ùå Missing | N/A | No tests enforcing task name stability |
| **#2: Worker DLQ** | DLQ table schema | ‚ùå Missing | N/A | No `celery_task_failures` or equivalent table |
| **#2: Worker DLQ** | Migration | ‚ùå Missing | N/A | No migration creating worker DLQ schema |
| **#2: Worker DLQ** | Failure hook | ‚ùå Missing | [celery_app.py:115-129](app/celery_app.py#L115-L129) | `task_failure` signal only logs/meters, doesn't persist to DLQ |
| **#2: Worker DLQ** | Retry mechanism | ‚ùå Missing | N/A | No task replay from DLQ |
| **#3: Observability** | Monitoring server | ‚úÖ Exists | [worker_monitoring.py](app/observability/worker_monitoring.py) | Infrastructure complete |
| **#3: Observability** | /metrics route | ‚úÖ Exists | [worker_monitoring.py:60-61](app/observability/worker_monitoring.py#L60-L61) | Route registered |
| **#3: Observability** | /health route | ‚úÖ Exists | [worker_monitoring.py:63-77](app/observability/worker_monitoring.py#L63-L77) | Route registered with broker+DB checks |
| **#3: Observability** | Metrics instrumentation | ‚úÖ Exists | [metrics.py:30-53](app/observability/metrics.py#L30-L53) | 4 metric families defined |
| **#3: Observability** | Signal hooks | ‚úÖ Exists | [celery_app.py:99-129](app/celery_app.py#L99-L129) | Wired to signals |
| **#3: Observability** | CI curl evidence | ‚ùå Missing | [ci.yml:164-169](.github/workflows/ci.yml#L164-L169) | CI runs tests but doesn't capture curl output as artifact |
| **#4: Minimal Logic** | Task complexity | ‚úÖ Satisfied | [tasks/housekeeping.py](app/tasks/housekeeping.py), [tasks/maintenance.py](app/tasks/maintenance.py), [tasks/llm.py](app/tasks/llm.py) | All tasks are stubs or infrastructure-verification |
| **#5: CI Soundness** | Postgres provisioning | ‚úÖ Complete | [ci.yml:92-105](.github/workflows/ci.yml#L92-L105) | Postgres-only service |
| **#5: CI Soundness** | Role provisioning | ‚úÖ Complete | [ci.yml:132-143](.github/workflows/ci.yml#L132-L143) | app_user, app_rw, app_ro created |
| **#5: CI Soundness** | Migrations | ‚úÖ Working | [ci.yml:151](.github/workflows/ci.yml#L151) | `alembic upgrade celery_foundation@head` succeeds |
| **#5: CI Soundness** | Worker start | ‚úÖ Working | [ci.yml:158](.github/workflows/ci.yml#L158) | Worker starts successfully |
| **#5: CI Soundness** | Test execution | ‚úÖ Working | [ci.yml:169](.github/workflows/ci.yml#L169) | Tests pass |
| **#5: CI Soundness** | Evidence artifacts | ‚ùå Missing | N/A | No artifact upload for curl outputs |

---

## 5. Open Questions

### 5.1 Queue Topology Design Decisions

**Question**: What is the intended queue topology for B0.5.2?

**Context**:
- Current: Default queue only (implicit)
- B0.4 precedent: Ingestion service uses synchronous request/response (no queuing)
- B0.5 intent: Background execution for attribution processing + matview refresh

**Options**:
1. **Single default queue**: Simple, no routing overhead
2. **Priority queues**: `default`, `priority`, `low` with routing by task criticality
3. **Domain queues**: `housekeeping`, `maintenance`, `llm`, `attribution` (future)

**Resolution Needed**: Before remediation can begin, queue topology must be specified.

---

### 5.2 Worker DLQ Scope

**Question**: Should worker DLQ capture all task failures or only tenant-scoped task failures?

**Context**:
- Ingestion DLQ: Captures failed webhook events (tenant-scoped)
- Worker tasks: Mix of global (refresh_all_materialized_views) and tenant-scoped (scan_for_pii_contamination)

**Options**:
1. **Tenant-scoped only**: DLQ table has tenant_id column, only @tenant_task failures captured
2. **All failures**: DLQ table has nullable tenant_id, captures all task failures

**Resolution Needed**: Affects DLQ schema design (tenant_id nullability, RLS policy).

---

### 5.3 CI Evidence Format

**Question**: What evidence format satisfies the B0.5.2 criterion?

**Context**:
- Criterion: "Evidence captured in CI logs or CI artifact bundle"
- Current: Test passes (indirect evidence) but no curl output in logs

**Options**:
1. **CI logs**: Add `curl -v` commands in CI steps, output appears in logs
2. **Artifact bundle**: Save curl outputs to files, upload as GitHub Actions artifact
3. **Both**: Logs for immediate visibility, artifacts for archival

**Preference**: Artifact bundle (more structured, easier to validate post-run).

---

### 5.4 Task Name Stability Enforcement

**Question**: Should task names be enforced via tests to prevent accidental renames?

**Context**:
- Task names are currently explicit (`name="app.tasks.housekeeping.ping"`)
- No test verifying task names remain stable across refactors

**Options**:
1. **Test-enforced**: Add test asserting expected task names in `celery_app.tasks.keys()`
2. **Convention-enforced**: Document naming convention in ADR, rely on code review
3. **No enforcement**: Accept task renames as non-breaking (require version bump)

**B0.5.2 Alignment**: Criterion requires "deterministic task registration with stable task names". Test enforcement recommended.

---

## 6. Architecture Integration Summary

### B0.1‚ÄìB0.4 Continuity

**Aligned**:
- ‚úÖ RLS + tenant GUC usage consistent with B0.3
- ‚úÖ Observability patterns (Prometheus metrics, JSON logs) consistent with B0.4
- ‚úÖ Postgres-first approach consistent with B0.3/B0.4
- ‚úÖ No Redis/Rabbit dependencies (aligned with margin protection)

**Not Integrated (by design)**:
- Contracts (B0.1): Worker tasks not exposed via API, no contract relevance
- Mock servers (B0.2): Worker infrastructure not mocked

---

### B0.5.1 Foundation Constraints

**Must Preserve**:
- Broker DSN construction: `sqla+postgresql://...`
- Result backend DSN construction: `db+postgresql://...`
- Worker monitoring server on CELERY_METRICS_PORT
- Signal hooks for metrics instrumentation
- Migration branch: `celery_foundation`

**Can Extend**:
- Add queue topology configuration (new config keys)
- Add worker DLQ table (new migration in 006_celery_foundation/)
- Add CI evidence capture steps (workflow changes only)

---

## 7. Remediation Readiness Assessment

### Blockers

None. All prerequisite infrastructure exists and is operational.

### Prerequisites Satisfied

- ‚úÖ B0.3 RLS + tenant GUC operational
- ‚úÖ B0.4 observability patterns established
- ‚úÖ B0.5.1 Celery foundation empirically complete
- ‚úÖ CI infrastructure sound (Postgres, roles, migrations, worker start)

### Next Steps (Not Executed, Awaiting Orchestrator Decision)

1. **Design queue topology**: Specify queue names, routing rules, worker binding strategy
2. **Design worker DLQ schema**: Table name, columns, tenant_id handling, RLS policy
3. **Implement queue topology**: Update celery_app.py with task_routes, task_queues
4. **Implement worker DLQ**: Create migration, add signal hook, implement capture logic
5. **Add CI evidence capture**: curl /health and /metrics, upload artifacts
6. **Validate with tests**: Test queue routing, DLQ capture, endpoint responses

---

## 8. Evidence Manifest

| Category | File Path | Line Reference | Purpose |
|----------|-----------|----------------|---------|
| **B0.1** | [contracts/README.md](contracts/README.md) | 1-114 | Contract infrastructure documentation |
| **B0.2** | [contracts/README.md](contracts/README.md) | 89-97 | Mock server port assignments |
| **B0.3** | [app/db/session.py](app/db/session.py) | 119-131 | `set_tenant_guc` helper function |
| **B0.3** | [alembic/versions/001_core_schema/202511131120_add_rls_policies.py](alembic/versions/001_core_schema/202511131120_add_rls_policies.py) | 1-end | RLS policies migration |
| **B0.4** | [app/observability/metrics.py](app/observability/metrics.py) | 1-54 | Metrics definitions |
| **B0.4** | [app/ingestion/dlq_handler.py](app/ingestion/dlq_handler.py) | 1-388 | Ingestion DLQ pattern |
| **B0.5.1** | [app/celery_app.py](app/celery_app.py) | 1-138 | Celery app configuration |
| **B0.5.1** | [app/observability/worker_monitoring.py](app/observability/worker_monitoring.py) | 1-114 | Worker monitoring server |
| **B0.5.1** | [alembic/versions/006_celery_foundation/202512120900_celery_tables.py](alembic/versions/006_celery_foundation/202512120900_celery_tables.py) | 1-89 | Celery tables migration |
| **Config** | [app/core/config.py](app/core/config.py) | 43-53 | Celery config settings |
| **Config** | [alembic.ini](alembic.ini) | 41 | Migration branch locations |
| **CI** | [.github/workflows/ci.yml](.github/workflows/ci.yml) | 88-177 | Celery foundation CI job |
| **Tests** | [tests/test_b051_celery_foundation.py](tests/test_b051_celery_foundation.py) | 1-191 | B0.5.1 validation tests |
| **Tasks** | [app/tasks/housekeeping.py](app/tasks/housekeeping.py) | 76-125 | Ping task (minimal domain logic) |
| **Tasks** | [app/tasks/maintenance.py](app/tasks/maintenance.py) | 1-186 | Maintenance stubs |
| **Tasks** | [app/tasks/llm.py](app/tasks/llm.py) | 1-82 | LLM stubs |
| **Tasks** | [app/tasks/context.py](app/tasks/context.py) | 36-75 | `@tenant_task` decorator |

---

## Document Completeness Checklist

- ‚úÖ Exit Gate 0: Baseline architecture mapped with file paths
- ‚úÖ Exit Gate 1: Queue topology inventoried (finding: missing)
- ‚úÖ Exit Gate 2: Task registration inventoried (finding: partial)
- ‚úÖ Exit Gate 3: Worker DLQ inventoried (finding: missing)
- ‚úÖ Exit Gate 4: Observability inventoried (finding: infrastructure complete, evidence pending)
- ‚úÖ Exit Gate 5: CI soundness inventoried (finding: working, evidence gaps)
- ‚úÖ Gap matrix compiled with evidence pointers
- ‚úÖ Open questions documented for orchestrator resolution
- ‚úÖ No proposed fixes (inventory only, as required)

---

**Inventory Complete**: 2025-12-13
**Status**: Awaiting orchestrator decision on remediation hypotheses and approach.
