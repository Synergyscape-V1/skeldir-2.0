# 4.1 Current Implementation Landscape
- **Celery app/config**: `backend/app/celery_app.py` builds broker/result URLs from `CELERY_BROKER_URL`/`CELERY_RESULT_BACKEND` or `DATABASE_URL` via `_sync_sqlalchemy_url`, setting `broker_url` to `sqla+postgresql://...` and `result_backend` to `db+postgresql://...`. Config includes JSON serialization, `task_track_started`, `broker_transport_options={"pool_recycle":300}`, `worker_send_task_events=True`, and structured logging via `configure_logging()` on `worker_process_init`. Metrics signals increment counters/histogram in `app.observability.metrics`. Autodiscovery is limited to `["app.tasks.housekeeping"]`.
- **Settings/env wiring**: `backend/app/core/config.py` defines optional `CELERY_BROKER_URL`/`CELERY_RESULT_BACKEND`, default pool sizes, and enforces `postgresql+asyncpg` or `postgresql` for `DATABASE_URL`.
- **DB session & tenant helpers**: `backend/app/db/session.py` provides an async `engine` (asyncpg) with pool sizing, `get_session` that sets `app.current_tenant_id`, and `set_tenant_guc` helper. Used by ingestion and by the Celery ping task to prove DB role.
- **Tasks registered**: `backend/app/tasks/housekeeping.py` defines `app.tasks.housekeeping.ping`, logging start/success, optional `fail=True`, optional `tenant_id` GUC, and returns `{"status":"ok","timestamp":..., "db_user": current_user}` using psycopg2 or async engine.
- **Task stubs not wired**: `backend/app/tasks/maintenance.py` declares shared tasks for matview refresh/PII scan/retention plus `BEAT_SCHEDULE`, but the module contains `async with` inside sync tasks (syntax error) and is not autodiscovered by `celery_app` (only housekeeping module is scanned). `DATABASE_URL` inside is a hard-coded placeholder, not the shared settings.
- **Observability hooks**: Celery counters/histogram defined in `backend/app/observability/metrics.py`; JSON logging formatter in `backend/app/observability/logging_config.py`; `/metrics` endpoint served by FastAPI in `backend/app/api/health.py` (exposes process metrics of the API process).
- **Integration with B0.4 ingestion**: Celery foundation reuses shared configuration (`app/core/config.py`), DB engine and tenant helpers (`app/db/session.py`), and observability modules (`app/observability/*`) used by ingestion/webhooks.
- **Runtime instructions**: `backend/README.md` documents running a worker with `python -m celery -A app.celery_app.celery_app worker -P solo -c 1 --loglevel=INFO`; notes Postgres-only broker/result backend and ping task validation.
- **Tests**: `backend/tests/test_b051_celery_foundation.py` sets default Neon `app_user` DSN, spawns a Celery worker, enqueues `ping`, asserts `celery_taskmeta` persistence and metrics in `/metrics` (eager mode), and checks structured logs.
- **Operational artifacts**: `Procfile` runs `redis-server` and `celery -A app.tasks worker` (Redis default broker, wrong app target); `.github/workflows/empirical-validation.yml` starts Redis service and `celery -A app.tasks worker`; `replit.nix` includes Redis. Binary baseline `backend/validation/evidence/runtime/binary_checks.txt` shows `celery` CLI missing on Windows at capture time. Temp probes (`backend/tmp_celery_init.py`, `tmp_celery_schema.py`, `tmp_list_tables.py`) were used to manually validate Celery init/schema creation.

# 4.2 Architectural Analysis
1) **Celery Architecture vs production-grade patterns**
- Broker/result configured for Postgres SQLAlchemy transport (`sqla+postgresql://`, `db+postgresql://`) derived from the same `DATABASE_URL` (`backend/app/celery_app.py`); no alternative broker allowed in code. Queue topology is implicit default queue; no routing, acks, or retry policy tuning. Concurrency/topology is dictated by the CLI invocation; README suggests `-P solo -c 1`, but no guard rails in config. Beat is not wired. Only housekeeping tasks are discovered; maintenance stubs are unreachable due to syntax error/autodiscovery scope.

2) **PostgreSQL broker performance & schema choices**
- Celery broker/result tables are created ad hoc by Celery in `public` (no Alembic migration or dedicated schema). With `app_user` DSN, DDL privileges are assumed but not provisioned by migrations (B0.3 docs note `app_user` is manual). No retention/cleanup for `celery_taskmeta`/`kombu_queue`, risking bloat. Transport options only set `pool_recycle`; connection pool sizing/prefetch are defaults, so Postgres polling overhead is unbounded relative to Neon pool limits.

3) **RLS & tenant isolation readiness**
- Tasks run under the database role from `DATABASE_URL` (tests expect `app_user`), but Celery broker/result operations do not set `app.current_tenant_id` and operate on non-RLS tables. Tenant context propagation is manual/optional (`tenant_id` arg in `housekeeping.ping`, `set_tenant_guc` helper); there is no decorator or middleware ensuring all tasks set the GUC. Since `app_user` isn’t created via migrations and may lack CREATE on `public`, Celery table creation can fail in locked-down environments.

4) **Observability integration**
- Worker logging configures JSON formatting on `worker_process_init` (`backend/app/celery_app.py`) but does not attach correlation/tenant context beyond what is already in contextvars (not set for Celery tasks). Metrics are emitted via Celery signals, yet workers expose no `/metrics` endpoint; only the FastAPI process serves Prometheus, so worker-side metrics are invisible unless tasks run eagerly inside the API process (as in the test). No worker health/readiness endpoints exist; CI process snapshots in `empirical-validation.yml` are passive.

# 4.3 Best Practice Compliance Assessment
- **Alignments**
  - Postgres-only broker/result backend with SQLAlchemy/database backends is centralized in `backend/app/celery_app.py`; env indirection is present via `CELERY_BROKER_URL`/`CELERY_RESULT_BACKEND`.
  - Structured logging and signal-based metrics hooks mirror B0.4 ingestion observability modules for consistency.
  - Ping task uses shared async engine and tenant GUC helper to validate DB role path, keeping business logic separate from Celery plumbing.
- **Critical deficiencies**
  - `backend/app/tasks/maintenance.py` is syntactically invalid (`async with` inside sync tasks; confirmed by `python -m py_compile ...` failure) and not autodiscovered, so scheduled maintenance/PII/retention tasks cannot load; it also hard-codes a placeholder DSN instead of settings.
  - Operational entrypoints (`Procfile`, `.github/workflows/empirical-validation.yml`) start `celery -A app.tasks worker` with Redis services, diverging from the Postgres-only Celery app (`app.celery_app.celery_app`), so the documented foundation is not what CI/Replit would run.
  - No migrations or schema ownership prepared for Celery tables; relying on `app_user` to auto-create `celery_taskmeta`/`kombu_*` can fail under RLS-friendly privilege sets and leaves infra tables unmanaged.
  - Worker metrics are not exported from actual worker processes; observability assertions in tests rely on eager mode within FastAPI, leaving production worker visibility absent.
  - Default DSNs in tests point at a shared Neon instance with embedded credentials; combined with missing `celery` binary in evidence (`backend/validation/evidence/runtime/binary_checks.txt`), the foundation is unvalidated and potentially non-reproducible.
- **Important issues**
  - Broker/result tables live in `public` with no retention policy, vacuum guidance, or indexing review; bloat risk grows with task volume.
  - Concurrency/prefetch/pool sizes are not bounded in config; CPU-count defaults could exhaust Neon connections when not run with the README’s `-c 1 -P solo`.
  - Logging lacks automatic correlation/tenant context for worker-emitted logs; tasks must set extras manually.
  - Autodiscovery excludes any future task packages, so new tasks require manual registration changes.
- **Cosmetic**
  - Mixed sync/async DB access inside `housekeeping.ping` adds complexity but is functionally harmless for the stub task.

# 4.4 Efficiency & Optimization Evaluation
- **DB connection usage**: Async engine pools default to size 10/overflow 20 per process (`backend/app/db/session.py`); each Celery worker process will instantiate its own pool. Celery SQLA transport opens additional sync connections without explicit pooling limits. Absent explicit `worker_concurrency` bounds, a default prefork worker could open tens of connections, exceeding Neon limits; README’s `-P solo -c 1` mitigates manually but is unenforced.
- **Broker table usage & retention**: Celery result backend writes to `celery_taskmeta`/`celery_tasksetmeta` in `public` with no TTL or cleanup process; queues via SQLA transport likewise accumulate rows. No maintenance tasks or vacuum guidance are present, so storage bloat and index drift are likely under sustained load.
- **Scalability within Postgres-only/75% margin**: The SQLAlchemy transport polls Postgres and is known to be resource-heavier than Redis/RabbitMQ. With no batching, retention, or connection caps, the current setup is suitable only for very low throughput; scaling beyond trivial volumes would pressure the primary application database and threaten the margin target.

# 4.5 Gap Analysis Against B0.5.x Completion Criteria
- **Infrastructure Operational**: `partially_implemented` — Celery app and ping task exist (`backend/app/celery_app.py`, `backend/app/tasks/housekeeping.py`), and a worker command is documented, but CI/Replit entrypoints still point to Redis/`app.tasks` and the `celery` binary was absent in evidence; Celery tables are unmanaged.
- **Tenant Isolation Enforced**: `partially_implemented` — Tenant GUC helper exists (`backend/app/db/session.py`) and ping can set it optionally, but there is no mandatory propagation mechanism, and broker/result operations run without RLS context; `app_user` role is not provisioned by migrations.
- **Stub Framework Active**: `not_implemented` — Only housekeeping is registered; LLM/task routing/explanation/budget stubs are absent, and `celery_app.autodiscover_tasks` excludes other packages.
- **Observability Delivered**: `partially_implemented` — Logging/metrics hooks are present in code, but worker metrics are not exposed via any endpoint and no worker health/readiness checks exist.
- **Deterministic Testing**: `implemented_but_unvalidated` — Targeted tests (`backend/tests/test_b051_celery_foundation.py`) exist but rely on a remote Neon DSN and a `celery` binary that was missing in captured environment; CI jobs do not provision Postgres/Celery in alignment with the Postgres broker.
- **Margin Guardrails Intact**: `partially_implemented` — Code enforces Postgres broker/backend, but operational scripts/workflows spin up Redis and an incorrect Celery app, undermining the Postgres-only/margin intent; no retention or connection caps are configured.
- **Operational Playbooks**: `not_implemented` — Only a single README command exists; no runbooks for worker lifecycle, rollback, or recovery are documented.

# 4.6 Hypothesis Validation & Conclusion
- **Hypothesis restated**: The B0.5.1 Celery foundation may be inefficient, suboptimal, or misaligned with the Product Vision (Postgres-only, RLS-first, reproducible CI/observability).
- **Conclusion**: The current B0.5.1 foundation has **architectural deficiencies that require remediation before B0.5.x can proceed safely**.
- **Evidence-backed rationale**:
  - Broker/result wiring is Postgres-only in code (`backend/app/celery_app.py`), but operational surfaces (Procfile, `empirical-validation.yml`, `replit.nix`) still orchestrate Redis and target `app.tasks`, so deployed workers would not match the designed foundation.
  - Core maintenance task module is syntactically broken and not registered (`backend/app/tasks/maintenance.py`), eliminating the supposed production-grade scaffolding (matview refresh/PII/retention).
  - Celery infrastructure tables are unmanaged (no migrations, default `public` schema) and may be unable to auto-create under least-privilege roles; no retention or connection bounds protect the primary database.
  - Observability depends on eager-mode execution in the API process for metrics exposure (`backend/tests/test_b051_celery_foundation.py`); actual workers expose no metrics/health endpoints.
  - Testing/CI evidence shows missing Celery binary and no Postgres-backed worker orchestration, so the foundation is unvalidated in reproducible environments.
- **Deficiencies by severity**
  - **Critical**: Broken maintenance task module; operational drift to Redis/incorrect app target; lack of managed schema/privileges for Celery tables; absence of worker metrics/health exposure; foundation unvalidated in CI (missing binary/real Postgres worker run).
  - **Important**: No enforced tenant GUC propagation; potential Postgres bloat from unbounded `celery_taskmeta`/queues; unbounded worker/broker connections relative to Neon limits; logs lack automatic correlation/tenant context in workers.
  - **Cosmetic**: Mixed sync/async DB access in `housekeeping.ping`; limited autodiscovery scope requiring manual updates for new task modules.
