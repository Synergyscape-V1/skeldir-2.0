# B0.5.1 Celery Runbook

## Start / Stop
- **Local**: `cd backend && CELERY_METRICS_PORT=9546 python -m celery -A app.celery_app.celery_app worker -P solo -c 1 --loglevel=INFO`
- **Procfile**: `worker` entry already targets `app.celery_app.celery_app` with Postgres broker/result.
- **CI**: see `.github/workflows/ci.yml` job `celery-foundation` (Postgres service + worker).
- **Stop**: `pkill -f "celery -A app.celery_app.celery_app"` or `kill $(cat /tmp/celery.pid)`.

## Graceful Shutdown
- Send **TERM** to the worker PID (not KILL). Celery drains in-flight tasks then exits with code 0.
- Verify drain: metrics at `http://localhost:${CELERY_METRICS_PORT}/metrics` should stop increasing; worker logs emit shutdown sequence.
- If stuck, send **KILL** only after TERM timeout; expect task losses.

## Health & Metrics
- **Metrics**: `http://<addr>:<CELERY_METRICS_PORT>/metrics` (Prometheus text). Exposes `celery_task_started_total`, `celery_task_success_total`, `celery_task_failure_total`, `celery_task_duration_seconds`.
- **Health**: `http://<addr>:<CELERY_METRICS_PORT>/health` checks broker (Postgres SQLA) and DB connectivity under `DATABASE_URL` (app_user).
- **Logging**: JSON via `app.observability.logging_config`; tenant/correlation context auto-populated for tenant-scoped tasks (`tenant_task` decorator).

## Concurrency & Pooling
- Default worker command uses `-P solo -c 1` to avoid exhausting Neon/Postgres pools. Increase cautiously; each process opens its own SQLAlchemy and Celery transport connections.
- DB pool defaults: pool_size=10, max_overflow=20 per process. Lower via env (`DATABASE_POOL_SIZE`, `DATABASE_MAX_OVERFLOW`) if running multiple workers.

## Celery Tables & Retention
- Tables provisioned by Alembic migration `006_celery_foundation/202512120900_celery_tables.py` in `public` schema (`celery_taskmeta`, `celery_tasksetmeta`, `kombu_message`, `kombu_queue`). Grants applied to `app_user` if it exists.
- **Bloat mitigation**: periodically purge old rows (e.g., `DELETE FROM celery_taskmeta WHERE date_done < now() - interval '7 days'; VACUUM ANALYZE celery_taskmeta;`). Run during off-peak; ensure no active tasks referencing the rows.

## Troubleshooting
- **Broker connection errors**: confirm `CELERY_BROKER_URL` is `sqla+postgresql://...` and Postgres reachable; check `/health` endpoint.
- **Result backend errors**: ensure Celery tables exist (`SELECT * FROM pg_tables WHERE tablename LIKE 'celery_%' OR tablename LIKE 'kombu_%';`) and `app_user` has privileges.
- **Pool exhaustion**: reduce `-c`, lower DB pool sizes, or add connection pooling limits in env.
- **Tenant/RLS issues**: tenant-scoped tasks must accept `tenant_id` and use `@tenant_task`; failures without tenant_id are intentional guardrails.
- **Metrics missing**: verify `CELERY_METRICS_PORT` not in use; restart worker after port change.

## Validation Snippets
- **Worker health**: `curl http://localhost:9546/health`
- **Metrics**: `curl http://localhost:9546/metrics | grep celery_task_success_total`
- **Schema check**: `psql "$DATABASE_URL" -c "SELECT schemaname, tablename FROM pg_tables WHERE tablename ILIKE 'celery%' OR tablename ILIKE 'kombu%';"`
- **Graceful drain demo**:
  1. Start worker, enqueue: `python - <<'PY'\nfrom app.tasks.housekeeping import ping; [ping.delay() for _ in range(3)]\nPY`
  2. `kill -TERM $(pgrep -f "celery -A app.celery_app.celery_app")`
  3. Confirm tasks finished via result backend or metrics.
