name: "R7: Final Winning State"

on:
  workflow_dispatch:
  push:
    branches: [main]
    paths:
      - "backend/app/**"
      - "alembic/**"
      - "scripts/r2/**"
      - "scripts/r3/**"
      - "scripts/r4/**"
      - "scripts/r5/**"
      - "scripts/r6/**"
      - ".github/workflows/r7-final-winning-state.yml"

jobs:
  r7-final-winning-state:
    runs-on: ubuntu-22.04
    timeout-minutes: 180
    env:
      CI: "true"
      PYTHONPATH: backend
      R7_PG_HOST: 127.0.0.1
      R7_PG_PORT: "5432"
      R7_PG_ADMIN_USER: r7_admin
      R7_PG_ADMIN_PASSWORD: r7_admin
      R7_PG_IMAGE: postgres:16
      R7_PG_CONTAINER: r7-postgres
      TENANT_API_KEY_HEADER: X-Skeldir-Tenant-Key
      CELERY_TASK_ACKS_LATE: "true"
      CELERY_TASK_REJECT_ON_WORKER_LOST: "true"
      CELERY_TASK_ACKS_ON_FAILURE_OR_TIMEOUT: "true"
      CELERY_WORKER_PREFETCH_MULTIPLIER: "1"
      CELERY_TASK_SOFT_TIME_LIMIT_S: "300"
      CELERY_TASK_TIME_LIMIT_S: "360"
      CELERY_WORKER_MAX_TASKS_PER_CHILD: "1"
      CELERY_WORKER_MAX_MEMORY_PER_CHILD_KB: "200000"
      CELERY_CHORD_UNLOCK_MAX_RETRIES: "5"
      CELERY_CHORD_UNLOCK_RETRY_DELAY_S: "2"
      CELERY_BROKER_ENGINE_POOL_SIZE: "2"
      CELERY_BROKER_ENGINE_MAX_OVERFLOW: "0"
      CELERY_RESULT_BACKEND_ENGINE_POOL_SIZE: "2"
      CELERY_BROKER_VISIBILITY_TIMEOUT_S: "10"
      CELERY_BROKER_RECOVERY_SWEEP_INTERVAL_S: "1.0"
      CELERY_BROKER_RECOVERY_TASK_NAME_FILTER: ""
      R6_WORKER_LOG_PATH: r6_worker.log
      R6_PROBE_LOG_PATH: r6_probe.log
      R5_DETERMINISM_EVENTS_N: "1000"
      R5_DETERMINISM_RUNS: "3"
      R5_DETERMINISM_CONCURRENCY: "10"
      R5_SCALE_N_SMALL: "10000"
      R5_SCALE_N_LARGE: "100000"
      ATTRIBUTION_BASELINE_BATCH_EVENTS: "100000"
      ENABLE_R5_RETRY_INJECTION: "1"

    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@82c7e631bb3cdc910f68e0081d67478d79c6982d
        with:
          python-version: "3.11"

      - name: Install dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt -r backend/requirements-dev.txt

      - name: "R7 Phase R0: Immutable fingerprint"
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p /tmp/r7_results
          echo "R7_PHASE_START=R0 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          docker pull "${R7_PG_IMAGE}"
          POSTGRES_DIGEST="$(docker inspect "${R7_PG_IMAGE}" --format '{{index .RepoDigests 0}}')"
          echo "POSTGRES_DIGEST=${POSTGRES_DIGEST}" >> "${GITHUB_ENV}"
          WORKFLOW_HASH="$(sha256sum .github/workflows/r7-final-winning-state.yml | cut -d' ' -f1)"
          python - <<'PY' > /tmp/r7_results/r0.json
          import json
          import os
          import platform
          import subprocess
          from datetime import datetime, timezone

          def _cmd(cmd):
              try:
                  return subprocess.check_output(cmd, stderr=subprocess.STDOUT).decode().strip()
              except Exception:
                  return "UNKNOWN"

          payload = {
              "phase": "R0",
              "status": "PASS",
              "candidate_sha": os.environ.get("GITHUB_SHA"),
              "git_sha": _cmd(["git", "rev-parse", "HEAD"]),
              "workflow_hash_sha256": _cmd(["sha256sum", ".github/workflows/r7-final-winning-state.yml"]).split()[0],
              "runner": {
                  "os": platform.platform(),
                  "arch": platform.machine(),
              },
              "toolchain": {
                  "python": _cmd(["python", "--version"]),
                  "node": _cmd(["bash", "-lc", "node --version || echo not_installed"]),
                  "npm": _cmd(["bash", "-lc", "npm --version || echo not_installed"]),
                  "docker": _cmd(["docker", "--version"]),
              },
              "images": {
                  "postgres": os.environ.get("POSTGRES_DIGEST"),
              },
              "captured_at_utc": datetime.now(timezone.utc).isoformat(),
          }
          print(json.dumps(payload, indent=2, sort_keys=True))
          PY
          echo "R7_R0_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r0.json
          echo "R7_R0_RESULT_JSON_END"
          echo "R7_PHASE_END=R0 status=PASS"

      - name: Start Postgres (shared for R1-R6)
        shell: bash
        run: |
          set -euo pipefail
          docker run -d --name "${R7_PG_CONTAINER}" \
            -p "${R7_PG_PORT}:5432" \
            --shm-size=1g \
            -e POSTGRES_USER="${R7_PG_ADMIN_USER}" \
            -e POSTGRES_PASSWORD="${R7_PG_ADMIN_PASSWORD}" \
            -e POSTGRES_DB=postgres \
            "${POSTGRES_DIGEST}" \
              -c log_statement=all \
              -c log_destination=stderr \
              -c logging_collector=off \
              -c shared_buffers=1GB \
              -c work_mem=32MB \
              -c maintenance_work_mem=256MB \
              -c max_wal_size=4GB \
              -c checkpoint_timeout=30min \
              -c checkpoint_completion_target=0.9 \
              -c wal_compression=on \
              -c fsync=off \
              -c synchronous_commit=off \
              -c full_page_writes=off \
              -c max_wal_senders=0 \
              -c wal_level=minimal

          for i in $(seq 1 60); do
            status="$(docker inspect -f '{{.State.Health.Status}}' "${R7_PG_CONTAINER}" 2>/dev/null || true)"
            if [ "$status" = "healthy" ]; then
              break
            fi
            sleep 2
          done

          docker exec "${R7_PG_CONTAINER}" psql -U "${R7_PG_ADMIN_USER}" -d postgres -c "SELECT 1" >/dev/null

      - name: Bootstrap databases and roles
        shell: bash
        run: |
          set -euo pipefail
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres <<'SQL'
          DO $$
          BEGIN
            IF NOT EXISTS (SELECT 1 FROM pg_roles WHERE rolname = 'app_user') THEN
              CREATE ROLE app_user LOGIN PASSWORD 'app_user' NOSUPERUSER NOCREATEDB NOCREATEROLE INHERIT;
            END IF;
          END
          $$;
          SQL

          for db in r1 r2 r3 r4 r5 r6; do
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres -c "DROP DATABASE IF EXISTS ${db};" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres -c "CREATE DATABASE ${db};" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d "${db}" -c "GRANT CONNECT ON DATABASE ${db} TO app_user;" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d "${db}" -c "GRANT USAGE ON SCHEMA public TO app_user;" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d "${db}" -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d "${db}" -c "GRANT USAGE ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d "${db}" -c "ALTER DEFAULT PRIVILEGES FOR ROLE ${R7_PG_ADMIN_USER} IN SCHEMA public GRANT SELECT, INSERT, UPDATE, DELETE ON TABLES TO app_user;" >/dev/null
            psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d "${db}" -c "ALTER DEFAULT PRIVILEGES FOR ROLE ${R7_PG_ADMIN_USER} IN SCHEMA public GRANT USAGE, SELECT ON SEQUENCES TO app_user;" >/dev/null
          done

      - name: "R7 Phase R1: Live invokable engine"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r1
          MIGRATION_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r1
        run: |
          set -euo pipefail
          echo "R7_PHASE_START=R1 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          alembic upgrade heads
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r1 -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r1 -c "GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null

          python - <<'PY'
          import app.main
          print("R1_APP_IMPORT_OK")
          PY

          pkill -f "uvicorn app.main:app" || true
          nohup python -m uvicorn app.main:app --host 127.0.0.1 --port 8000 > /tmp/r7_uvicorn_r1.log 2>&1 &
          API_PID=$!
          sleep 1
          if ! kill -0 "${API_PID}" 2>/dev/null; then
            echo "R1 API process exited immediately"
            echo "---- r7_uvicorn_r1.log ----"
            tail -n 200 /tmp/r7_uvicorn_r1.log || true
            exit 1
          fi

          for i in $(seq 1 60); do
            if curl -fsS http://127.0.0.1:8000/health >/dev/null; then
              break
            fi
            sleep 1
          done

          health_code="$(curl -s -o /tmp/r7_health.json -w "%{http_code}" http://127.0.0.1:8000/health || true)"
          metrics_code="$(curl -s -o /tmp/r7_metrics.txt -w "%{http_code}" http://127.0.0.1:8000/metrics || true)"
          openapi_code="$(curl -s -o /tmp/r7_openapi.json -w "%{http_code}" http://127.0.0.1:8000/openapi.json || true)"
          export HEALTH_CODE="${health_code}"
          export METRICS_CODE="${metrics_code}"
          export OPENAPI_CODE="${openapi_code}"

          kill "${API_PID}" || true

          if [ "${health_code}" != "200" ] || [ "${metrics_code}" != "200" ] || [ "${openapi_code}" != "200" ]; then
            echo "R1 health/metrics/openapi checks failed: health=${health_code} metrics=${metrics_code} openapi=${openapi_code}"
            echo "---- r7_uvicorn_r1.log ----"
            tail -n 200 /tmp/r7_uvicorn_r1.log || true
            exit 1
          fi

          python - <<'PY' > /tmp/r7_results/r1.json
          import json
          from pathlib import Path

          import os

          health_code = int(os.environ.get("HEALTH_CODE", "0") or 0)
          metrics_code = int(os.environ.get("METRICS_CODE", "0") or 0)
          openapi_code = int(os.environ.get("OPENAPI_CODE", "0") or 0)
          metrics_len = len(Path("/tmp/r7_metrics.txt").read_text()) if Path("/tmp/r7_metrics.txt").exists() else 0
          openapi_len = len(Path("/tmp/r7_openapi.json").read_text()) if Path("/tmp/r7_openapi.json").exists() else 0
          payload = {
              "phase": "R1",
              "status": "PASS",
              "health_status": health_code,
              "metrics_status": metrics_code,
              "metrics_length": metrics_len,
              "openapi_status": openapi_code,
              "openapi_length": openapi_len,
          }
          print(json.dumps(payload, indent=2, sort_keys=True))
          PY

          echo "R7_R1_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r1.json
          echo "R7_R1_RESULT_JSON_END"
          echo "R7_PHASE_END=R1 status=PASS"

      - name: "R7 Phase R2: Data-Truth hardening"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r2
          MIGRATION_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r2
        run: |
          set -euo pipefail
          echo "R7_PHASE_START=R2 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          alembic upgrade heads
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r2 -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r2 -c "GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres -c "ALTER SYSTEM SET log_statement='all';" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres -c "SELECT pg_reload_conf();" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r2 <<'SQL'
          INSERT INTO tenants (id, name, api_key_hash, notification_email, created_at, updated_at)
          VALUES ('11111111-1111-1111-1111-111111111111', 'R2 Tenant A', 'hash_a', 'a@test.invalid', NOW(), NOW())
          ON CONFLICT (id) DO NOTHING;
          INSERT INTO channel_taxonomy (code, family, is_paid, display_name, is_active, state, created_at)
          VALUES
            ('unknown', 'unknown', false, 'Unknown', true, 'active', NOW()),
            ('direct', 'direct', false, 'Direct', true, 'active', NOW()),
            ('organic', 'organic', false, 'Organic', true, 'active', NOW()),
            ('referral', 'referral', false, 'Referral', true, 'active', NOW()),
            ('email', 'email', false, 'Email', true, 'active', NOW())
          ON CONFLICT (code) DO NOTHING;
          SQL

          LOG_SINCE="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          echo "R2_LOG_SINCE=${LOG_SINCE}"
          WINDOW_ID="r7_${GITHUB_RUN_ID}_${GITHUB_RUN_ATTEMPT}"
          python scripts/r2/runtime_scenario_suite.py \
            --candidate-sha "${GITHUB_SHA}" \
            --window-id "${WINDOW_ID}" \
            --orm-verdict-json /tmp/r2_orm.json | tee /tmp/r2_scenario.log

          sleep 2
          docker logs --since "${LOG_SINCE}" "${R7_PG_CONTAINER}" > /tmp/r7_postgres.log 2>&1

          python scripts/r2/db_log_window_audit.py \
            --log-file /tmp/r7_postgres.log \
            --candidate-sha "${GITHUB_SHA}" \
            --window-id "${WINDOW_ID}" \
            --num-scenarios 6 \
            --artifact-json /tmp/r2_db.json | tee /tmp/r2_db.log

          python scripts/r2/orm_window_audit.py \
            --orm-window-json /tmp/r2_orm.json \
            --num-scenarios 6 | tee /tmp/r2_orm.log

          python scripts/r2/instrument_consistency_gate.py \
            --db-window-json /tmp/r2_db.json \
            --orm-window-json /tmp/r2_orm.json \
            --num-scenarios 6 | tee /tmp/r2_consistency.log

          python - <<'PY' > /tmp/r7_results/r2.json
          import json
          from pathlib import Path

          db = json.loads(Path("/tmp/r2_db.json").read_text())
          orm = json.loads(Path("/tmp/r2_orm.json").read_text())
          payload = {
              "phase": "R2",
              "status": "PASS",
              "db_window": {
                  "total_db_statements_captured_in_window": db.get("total_db_statements_captured_in_window"),
                  "match_count_destructive_on_immutable": db.get("match_count_destructive_on_immutable"),
                  "failures": db.get("failures", []),
              },
              "orm_window": {
                  "total_orm_statements_captured_in_window": orm.get("total_orm_statements_captured_in_window"),
                  "orm_forbidden_match_count": orm.get("orm_forbidden_match_count"),
                  "failures": orm.get("failures", []),
              },
          }
          if payload["db_window"]["failures"] or payload["orm_window"]["failures"]:
              payload["status"] = "FAIL"
          print(json.dumps(payload, indent=2, sort_keys=True))
          if payload["status"] != "PASS":
              raise SystemExit(1)
          PY

          echo "R7_R2_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r2.json
          echo "R7_R2_RESULT_JSON_END"
          echo "R7_PHASE_END=R2 status=PASS"

      - name: "R7 Phase R3a: Ingestion migrate"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r3
          MIGRATION_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r3
          DATABASE_POOL_SIZE: "20"
          DATABASE_MAX_OVERFLOW: "20"
          LOG_LEVEL: "DEBUG"
        run: |
          set -euo pipefail
          echo "R7_PHASE_START=R3 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          ulimit -n 65535 || true
          alembic upgrade heads
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r3 -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r3 -c "GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null
          echo "R3_MIGRATIONS_OK"

      - name: "R7 Phase R3b: Ingestion API start"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r3
          R3_API_BASE_URL: http://127.0.0.1:8000
          DATABASE_POOL_SIZE: "20"
          DATABASE_MAX_OVERFLOW: "20"
          LOG_LEVEL: "DEBUG"
        run: |
          set -euo pipefail
          pkill -f "uvicorn app.main:app" || true
          nohup env PYTHONUNBUFFERED=1 python -u -m uvicorn app.main:app \
            --host 127.0.0.1 \
            --port 8000 \
            --log-level debug \
            --access-log \
            --workers 2 \
            --backlog 2048 > /tmp/r7_uvicorn_r3.log 2>&1 &
          API_PID=$!
          echo "R3_API_PID=${API_PID}" >> "${GITHUB_ENV}"
          sleep 1
          if ! kill -0 "${API_PID}" 2>/dev/null; then
            echo "R3 API process exited immediately"
            ls -l /tmp/r7_uvicorn_r3.log || true
            tail -n 200 /tmp/r7_uvicorn_r3.log || true
            exit 1
          fi
          for i in $(seq 1 60); do
            if curl -fsS http://127.0.0.1:8000/health/ready >/dev/null; then
              break
            fi
            sleep 1
          done
          if ! curl -fsS http://127.0.0.1:8000/health/ready >/dev/null; then
            echo "R3 API failed readiness checks"
            ps -p "${API_PID}" -o pid,ppid,cmd || true
            ss -ltnp | grep ":8000" || true
            ls -l /tmp/r7_uvicorn_r3.log || true
            tail -n 200 /tmp/r7_uvicorn_r3.log || true
            exit 1
          fi
          echo "R3_API_READY"

      - name: "R7 Phase R3c: Ingestion harness"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r3
          R3_DATABASE_URL: postgresql://app_user:app_user@127.0.0.1:5432/r3
          R3_API_BASE_URL: http://127.0.0.1:8000
          R3_LADDER: "50,250,1000"
          R3_CONCURRENCY: "200"
          R3_TIMEOUT_S: "30"
          DATABASE_POOL_SIZE: "20"
          DATABASE_MAX_OVERFLOW: "20"
          LOG_LEVEL: "DEBUG"
        run: |
          set -euo pipefail
          python scripts/r3/ingestion_under_fire.py | tee /tmp/r3_harness.log
          R3_HARNESS_EXIT=${PIPESTATUS[0]}
          export R3_HARNESS_EXIT
          if [ -n "${R3_API_PID:-}" ]; then
            kill "${R3_API_PID}" || true
          else
            pkill -f "uvicorn app.main:app" || true
          fi

          python - <<'PY' > /tmp/r7_results/r3.json
          import json
          import os
          from pathlib import Path

          scenarios = []
          current = None
          lines = Path("/tmp/r3_harness.log").read_text().splitlines()
          for line in lines:
              if line.startswith("R3_VERDICT_BEGIN"):
                  current = {"name": line.split(" ", 1)[1].strip(), "json": ""}
                  continue
              if line.startswith("R3_VERDICT_END"):
                  if current and current["json"]:
                      scenarios.append(json.loads(current["json"]))
                  current = None
                  continue
              if current is not None:
                  current["json"] += line + "\n"
          exit_code = int(os.environ.get("R3_HARNESS_EXIT", "1") or 1)
          passed = bool(scenarios) and exit_code == 0 and all(s.get("passed") for s in scenarios)
          payload = {
              "phase": "R3",
              "status": "PASS" if passed else "FAIL",
              "harness_exit_code": exit_code,
              "scenarios_executed": len(scenarios),
              "scenarios": scenarios,
          }
          print(json.dumps(payload, indent=2, sort_keys=True))
          PY

          R3_STATUS=$(python - <<'PY'
          import json
          from pathlib import Path
          payload = json.loads(Path("/tmp/r7_results/r3.json").read_text())
          print(payload.get("status", "FAIL"))
          PY
          )
          if [ "${R3_STATUS}" != "PASS" ]; then
            echo "R3 verdict failed; dumping uvicorn log"
            if [ -n "${R3_API_PID:-}" ]; then
              ps -p "${R3_API_PID}" -o pid,ppid,cmd || true
            fi
            ss -ltnp | grep ":8000" || true
            ls -l /tmp/r7_uvicorn_r3.log || true
            tail -n 200 /tmp/r7_uvicorn_r3.log || true
            exit 1
          fi

          echo "R7_R3_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r3.json
          echo "R7_R3_RESULT_JSON_END"
          echo "R7_PHASE_END=R3 status=PASS"

      - name: "R7 Phase R4a: Worker failure semantics prep"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r4
          MIGRATION_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r4
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r4
          R4_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
        run: |
          set -euo pipefail
          echo "R7_PHASE_START=R4 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          mkdir -p tmp/r4_results
          alembic upgrade heads
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r4 -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r4 -c "GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null

          pkill -f "uvicorn app.main:app" || true
          nohup python -m uvicorn app.main:app --host 127.0.0.1 --port 8000 > /tmp/r7_uvicorn_r4.log 2>&1 &
          for i in $(seq 1 60); do
            if curl -fsS http://127.0.0.1:8000/health >/dev/null; then
              break
            fi
            sleep 1
          done
          if ! curl -fsS http://127.0.0.1:8000/health >/dev/null; then
            echo "R4 API failed to become healthy"
            tail -n 200 /tmp/r7_uvicorn_r4.log || true
            exit 1
          fi

      - name: "R7 Phase R4b: Poison pill"
        shell: bash
        continue-on-error: true
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r4
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r4
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r4
          R4_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
          RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          R4_POISON_N: "10"
          R4_WORKER_CONCURRENCY: "2"
          R4_WORKER_POOL: "prefork"
          R4_POISON_WORKER_POOL: "threads"
        run: |
          set -euo pipefail
          python scripts/r4/worker_failure_semantics.py --scenario S1 --output-json tmp/r4_results/r4_s1.json

      - name: "R7 Phase R4c: Crash after write"
        shell: bash
        continue-on-error: true
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r4
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r4
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r4
          R4_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
          RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          R4_CRASH_N: "10"
          R4_WORKER_CONCURRENCY: "2"
          R4_WORKER_POOL: "prefork"
          R4_CRASH_WORKER_POOL: "prefork"
          R4_CRASH_WORKER_CONCURRENCY: "1"
        run: |
          set -euo pipefail
          python scripts/r4/worker_failure_semantics.py --scenario S2 --output-json tmp/r4_results/r4_s2.json

      - name: "R7 Phase R4d: RLS probe"
        shell: bash
        continue-on-error: true
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r4
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r4
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r4
          R4_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
          RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          R4_WORKER_CONCURRENCY: "2"
          R4_WORKER_POOL: "prefork"
        run: |
          set -euo pipefail
          python scripts/r4/worker_failure_semantics.py --scenario S3 --output-json tmp/r4_results/r4_s3.json

      - name: "R7 Phase R4e: Runaway no-starve"
        shell: bash
        continue-on-error: true
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r4
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r4
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r4
          R4_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
          RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          R4_SENTINEL_N: "10"
          R4_WORKER_CONCURRENCY: "2"
          R4_WORKER_POOL: "prefork"
        run: |
          set -euo pipefail
          python scripts/r4/worker_failure_semantics.py --scenario S4 --output-json tmp/r4_results/r4_s4.json

      - name: "R7 Phase R4f: Least privilege"
        shell: bash
        continue-on-error: true
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r4
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r4
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r4
          R4_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r4
          RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
          R4_WORKER_CONCURRENCY: "2"
          R4_WORKER_POOL: "prefork"
        run: |
          set -euo pipefail
          python scripts/r4/worker_failure_semantics.py --scenario S5 --output-json tmp/r4_results/r4_s5.json

      - name: "R7 Phase R4g: Aggregate"
        shell: bash
        if: always()
        run: |
          set -euo pipefail
          python - <<'PY' > /tmp/r7_results/r4.json
          import json
          from pathlib import Path

          result_files = {
              "S1": Path("tmp/r4_results/r4_s1.json"),
              "S2": Path("tmp/r4_results/r4_s2.json"),
              "S3": Path("tmp/r4_results/r4_s3.json"),
              "S4": Path("tmp/r4_results/r4_s4.json"),
              "S5": Path("tmp/r4_results/r4_s5.json"),
          }
          results = {}
          errors = {}
          missing = []
          for key, path in result_files.items():
              if not path.exists():
                  missing.append(key)
                  continue
              try:
                  results[key] = json.loads(path.read_text())
              except Exception as exc:  # noqa: BLE001
                  errors[key] = f"{exc.__class__.__name__}: {exc}"
          failed = [k for k, r in results.items() if not r.get("passed")]
          passed = not missing and not failed and not errors
          payload = {
              "phase": "R4",
              "status": "PASS" if passed else "FAIL",
              "missing": missing,
              "failed": failed,
              "errors": errors,
              "results": results,
          }
          print(json.dumps(payload, indent=2, sort_keys=True))
          if missing:
              print(
                  f"::error file=.github/workflows/r7-final-winning-state.yml,line=1,col=1::"
                  f"R4_MISSING_RESULTS={','.join(missing)}"
              )
          if failed:
              print(
                  f"::error file=.github/workflows/r7-final-winning-state.yml,line=1,col=1::"
                  f"R4_FAILED_SCENARIOS={','.join(failed)}"
              )
          if errors:
              print(
                  f"::error file=.github/workflows/r7-final-winning-state.yml,line=1,col=1::"
                  f"R4_RESULT_PARSE_ERRORS={','.join(errors.keys())}"
              )
          if not passed:
              raise SystemExit(1)
          PY

          echo "R7_R4_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r4.json
          echo "R7_R4_RESULT_JSON_END"
          echo "R7_PHASE_END=R4 status=PASS"
          pkill -f "uvicorn app.main:app" || true
          pkill -f "celery -A app.celery_app.celery_app worker" || true

      - name: "R7 Phase R5: Determinism + scaling"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r5
          MIGRATION_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r5
          R5_ADMIN_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r5
        run: |
          set -euo pipefail
          echo "R7_PHASE_START=R5 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          alembic upgrade heads
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r5 -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r5 -c "GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres -c "ALTER SYSTEM SET log_statement='none';" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d postgres -c "SELECT pg_reload_conf();" >/dev/null

          python scripts/r5/r5_verification.py | tee /tmp/r5_verification.log

          python - <<'PY' > /tmp/r7_results/r5.json
          import json
          from pathlib import Path

          lines = Path("/tmp/r5_verification.log").read_text().splitlines()
          capture = False
          buf = []
          for line in lines:
              if line.strip() == "R5_VERDICT_BEGIN":
                  capture = True
                  continue
              if line.strip() == "R5_VERDICT_END":
                  break
              if capture:
                  buf.append(line)
          verdict = json.loads("\n".join(buf)) if buf else {}
          status = "PASS" if verdict.get("pass") else "FAIL"
          payload = {"phase": "R5", "status": status, "verdict": verdict}
          print(json.dumps(payload, indent=2, sort_keys=True))
          if status != "PASS":
              raise SystemExit(1)
          PY

          echo "R7_R5_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r5.json
          echo "R7_R5_RESULT_JSON_END"
          echo "R7_PHASE_END=R5 status=PASS"

      - name: "R7 Phase R6: Governance under load"
        shell: bash
        env:
          DATABASE_URL: postgresql+asyncpg://app_user:app_user@127.0.0.1:5432/r6
          MIGRATION_DATABASE_URL: postgresql://r7_admin:r7_admin@127.0.0.1:5432/r6
          CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/r6
          CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/r6
          R6_SHA: ${{ github.sha }}
          R6_RUN_URL: https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}
        run: |
          set -euo pipefail
          echo "R7_PHASE_START=R6 ts=$(date -u +%Y-%m-%dT%H:%M:%SZ) sha=${GITHUB_SHA}"
          alembic upgrade heads
          export PGPASSWORD="${R7_PG_ADMIN_PASSWORD}"
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r6 -c "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_user;" >/dev/null
          psql -h "${R7_PG_HOST}" -U "${R7_PG_ADMIN_USER}" -d r6 -c "GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_user;" >/dev/null

          nohup celery -A app.celery_app.celery_app worker \
            --loglevel=INFO \
            --pool=prefork \
            --concurrency=2 \
            -Q housekeeping,maintenance,llm,attribution \
            --max-tasks-per-child "${CELERY_WORKER_MAX_TASKS_PER_CHILD}" \
            --max-memory-per-child "${CELERY_WORKER_MAX_MEMORY_PER_CHILD_KB}" \
            --logfile r6_worker.log \
            --pidfile /tmp/r6_worker.pid >/dev/null 2>&1 &
          sleep 5

          python scripts/r6/r6_context_gathering.py

          base="docs/validation/runtime/R6_context_gathering/${GITHUB_SHA}"
          echo "R6_RUNTIME_SNAPSHOT_JSON_BEGIN"
          cat "$base/R6_CELERY_INSPECT_CONF.json"
          echo "R6_RUNTIME_SNAPSHOT_JSON_END"
          echo "R6_PROBE_TIMEOUT_JSON_BEGIN"
          cat "$base/R6_PROBE_TIMEOUT.json"
          echo "R6_PROBE_TIMEOUT_JSON_END"
          echo "R6_PROBE_RETRY_JSON_BEGIN"
          cat "$base/R6_PROBE_RETRY.json"
          echo "R6_PROBE_RETRY_JSON_END"
          echo "R6_PROBE_PREFETCH_JSON_BEGIN"
          cat "$base/R6_PROBE_PREFETCH.json"
          echo "R6_PROBE_PREFETCH_JSON_END"
          echo "R6_PROBE_RECYCLE_JSON_BEGIN"
          cat "$base/R6_PROBE_RECYCLE.json"
          echo "R6_PROBE_RECYCLE_JSON_END"
          echo "R6_PROBE_LOG_BEGIN"
          cat r6_probe.log || true
          echo "R6_PROBE_LOG_END"

          python - <<'PY' > /tmp/r7_results/r6.json
          import json
          import os
          from pathlib import Path

          base = Path("docs/validation/runtime/R6_context_gathering") / os.environ["GITHUB_SHA"]
          env = json.loads((base / "R6_ENV_SNAPSHOT.json").read_text())
          conf = json.loads((base / "R6_CELERY_INSPECT_CONF.json").read_text())
          timeout = json.loads((base / "R6_PROBE_TIMEOUT.json").read_text())
          retry = json.loads((base / "R6_PROBE_RETRY.json").read_text())
          prefetch = json.loads((base / "R6_PROBE_PREFETCH.json").read_text())
          recycle = json.loads((base / "R6_PROBE_RECYCLE.json").read_text())

          pid_samples = recycle.get("pid_samples") or []
          unique_pids = sorted(set(pid_samples))
          derived_checks = recycle.get("derived_checks") or {}

          status = "PASS"
          if not timeout.get("soft_limit_observed") or not timeout.get("hard_limit_observed"):
              status = "FAIL"
          if not retry.get("attempt_numbers"):
              status = "FAIL"
          if not prefetch.get("probe_valid"):
              status = "FAIL"
          if recycle.get("unique_pid_count") != len(unique_pids):
              status = "FAIL"
          if recycle.get("unique_pids") != unique_pids:
              status = "FAIL"
          if not derived_checks.get("assert_unique_pid_count_matches_samples"):
              status = "FAIL"
          if not derived_checks.get("assert_unique_pids_matches_samples"):
              status = "FAIL"

          payload = {
              "phase": "R6",
              "status": status,
              "env": {"sha": env.get("R6_SHA"), "github_sha": env.get("github_sha")},
              "runtime_conf": conf.get("conf", {}),
              "timeout": timeout,
              "retry": retry,
              "prefetch": prefetch,
              "recycle": recycle,
          }
          print(json.dumps(payload, indent=2, sort_keys=True))
          if status != "PASS":
              raise SystemExit(1)
          PY

          echo "R7_R6_RESULT_JSON_BEGIN"
          cat /tmp/r7_results/r6.json
          echo "R7_R6_RESULT_JSON_END"
          echo "R7_PHASE_END=R6 status=PASS"
          if [ -f /tmp/r6_worker.pid ]; then
            kill "$(cat /tmp/r6_worker.pid)" || true
          fi

      - name: "R7 Final verdict"
        shell: bash
        run: |
          set -euo pipefail
          python - <<'PY'
          import json
          from pathlib import Path

          results = {}
          for phase in ("r0","r1","r2","r3","r4","r5","r6"):
              path = Path("/tmp/r7_results") / f"{phase}.json"
              if not path.exists():
                  raise SystemExit(f"Missing result file: {path}")
              results[phase] = json.loads(path.read_text())
          if any(results[p]["status"] != "PASS" for p in results):
              raise SystemExit("R7_FINAL_VERDICT=FAIL")
          print("R7_FINAL_VERDICT=PASS")
          PY
