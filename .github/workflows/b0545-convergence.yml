name: b0545-convergence

on:
  pull_request:
    paths:
      - ".github/workflows/b0545-convergence.yml"
      - "backend/**"
      - "alembic/**"
      - "scripts/**"
  workflow_dispatch:

jobs:
  b0545-convergence:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: postgres
        ports:
          - 5432:5432
        options: >-
          --health-cmd "pg_isready -U postgres -d postgres"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql://app_user:app_user@127.0.0.1:5432/skeldir_validation
      MIGRATION_DATABASE_URL: postgresql://app_user:app_user@127.0.0.1:5432/skeldir_validation
      CELERY_BROKER_URL: sqla+postgresql://app_user:app_user@127.0.0.1:5432/skeldir_validation
      CELERY_RESULT_BACKEND: db+postgresql://app_user:app_user@127.0.0.1:5432/skeldir_validation
      CELERY_METRICS_PORT: "9546"
      CELERY_METRICS_ADDR: "127.0.0.1"
      ZG_BEAT_TEST_INTERVAL_SECONDS: "5"
      PYTHONPATH: backend
      CI: "true"
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Remove repo .env for CI isolation
        run: rm -f .env

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r backend/requirements.txt -r backend/requirements-dev.txt

      - name: Install psql client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Prepare database and roles
        run: |
          set -euo pipefail
          echo "Creating database roles and skeldir_validation database..."
          PGPASSWORD=postgres psql -h localhost -U postgres -c "CREATE USER app_user WITH PASSWORD 'app_user';"
          PGPASSWORD=postgres psql -h localhost -U postgres -c "CREATE USER app_rw WITH PASSWORD 'app_rw';"
          PGPASSWORD=postgres psql -h localhost -U postgres -c "CREATE USER app_ro WITH PASSWORD 'app_ro';"
          PGPASSWORD=postgres psql -h localhost -U postgres -c "CREATE DATABASE skeldir_validation OWNER app_user;"
          PGPASSWORD=postgres psql -h localhost -U postgres -c "GRANT ALL PRIVILEGES ON DATABASE skeldir_validation TO app_user;"
          PGPASSWORD=postgres psql -h localhost -U postgres -d skeldir_validation -c "GRANT ALL ON SCHEMA public TO app_user;"
          PGPASSWORD=postgres psql -h localhost -U postgres -d skeldir_validation -c "GRANT ALL ON SCHEMA public TO app_rw;"
          PGPASSWORD=postgres psql -h localhost -U postgres -d skeldir_validation -c "GRANT USAGE ON SCHEMA public TO app_ro;"
          echo "Database setup complete"

      - name: Run migrations
        run: |
          set -euo pipefail
          alembic upgrade 202511131121
          alembic upgrade skeldir_foundation@head

      - name: Seed tenant for matview pulse
        run: |
          set -euo pipefail
          psql "${DATABASE_URL}" -c "INSERT INTO tenants (id, name, api_key_hash, notification_email) VALUES ('11111111-1111-1111-1111-111111111111', 'B0545 Tenant', 'B0545_11111111-1111-1111-1111-111111111111', 'b0545-tenant@example.com') ON CONFLICT (id) DO NOTHING;"

      - name: Start worker + beat and capture kinetic evidence
        run: |
          set -euo pipefail
          EVIDENCE_DIR="${GITHUB_WORKSPACE}/evidence/b0545_ci"
          mkdir -p "${EVIDENCE_DIR}"
          printf "sha=%s\nworkflow=%s\n" "${GITHUB_SHA}" "${GITHUB_WORKFLOW}" > "${EVIDENCE_DIR}/metadata.txt"
          printf "database_url=%s\ncelery_broker_url=%s\ncelery_result_backend=%s\n" \
            "${DATABASE_URL}" "${CELERY_BROKER_URL}" "${CELERY_RESULT_BACKEND}" \
            | sed 's/:[^@]*@/:***@/' >> "${EVIDENCE_DIR}/metadata.txt"

          export PYTHONPATH="${GITHUB_WORKSPACE}/backend:${PYTHONPATH}"

          PROM_DIR="${EVIDENCE_DIR}/prom_multiproc"
          mkdir -p "${PROM_DIR}"
          chmod 700 "${PROM_DIR}"
          export PROMETHEUS_MULTIPROC_DIR="${PROM_DIR}"

          python - <<'PY' > "${EVIDENCE_DIR}/celery_config.json"
          import json
          from sqlalchemy.engine.url import make_url
          from app.celery_app import celery_app

          def redact(url: str | None) -> str:
              if not url:
                  return ""
              try:
                  parsed = make_url(url)
                  if parsed.password:
                      parsed = parsed.set(password="***")
                  return str(parsed)
              except Exception:
                  return url

          payload = {
              "broker_url": redact(celery_app.conf.broker_url),
              "result_backend": redact(celery_app.conf.result_backend),
              "beat_schedule_keys": list(celery_app.conf.beat_schedule.keys()) if celery_app.conf.beat_schedule else [],
          }
          print(json.dumps(payload, indent=2))
          PY

          psql "${DATABASE_URL}" -c "select count(*) from worker_failed_jobs;" > "${EVIDENCE_DIR}/worker_failed_jobs_before.txt"
          psql "${DATABASE_URL}" -c "select count(*) from kombu_message;" > "${EVIDENCE_DIR}/kombu_message_before.txt"

          WORKER_NODE="b0545@$(hostname)"
          stdbuf -oL -eL celery -A app.celery_app.celery_app worker -n "${WORKER_NODE}" -P solo -c 1 --loglevel=INFO \
            > "${EVIDENCE_DIR}/worker.log" 2>&1 &
          echo $! > "${EVIDENCE_DIR}/worker.pid"

          for i in $(seq 1 20); do
            celery -A app.celery_app.celery_app inspect ping --timeout=10 --destination "${WORKER_NODE}" \
              > "${EVIDENCE_DIR}/inspect_ping.txt" 2>&1 || true
            if grep -E -q "(pong|OK)" "${EVIDENCE_DIR}/inspect_ping.txt"; then
              break
            fi
            sleep 2
          done
          cat "${EVIDENCE_DIR}/inspect_ping.txt" || true

          celery -A app.celery_app.celery_app inspect registered --destination "${WORKER_NODE}" \
            > "${EVIDENCE_DIR}/inspect_registered.txt" 2>&1 || true
          celery -A app.celery_app.celery_app inspect active_queues --destination "${WORKER_NODE}" \
            > "${EVIDENCE_DIR}/inspect_active_queues.txt" 2>&1 || true
          celery -A app.celery_app.celery_app inspect stats --destination "${WORKER_NODE}" \
            > "${EVIDENCE_DIR}/inspect_stats.txt" 2>&1 || true

          stdbuf -oL -eL celery -A app.celery_app.celery_app beat --loglevel=INFO \
            > "${EVIDENCE_DIR}/beat.log" 2>&1 &
          echo $! > "${EVIDENCE_DIR}/beat.pid"

          for i in $(seq 1 30); do
            if grep -q "Sending due task" "${EVIDENCE_DIR}/beat.log"; then
              break
            fi
            sleep 2
          done
          grep -n "Sending due task" "${EVIDENCE_DIR}/beat.log" | tail -n 5 > "${EVIDENCE_DIR}/beat_due_task.txt" || true
          grep -n "pulse_matviews_global" "${EVIDENCE_DIR}/beat.log" | tail -n 10 > "${EVIDENCE_DIR}/beat_pulse_task.txt" || true

          for i in $(seq 1 30); do
            if grep -q "matview_pulse_task_start" "${EVIDENCE_DIR}/worker.log"; then
              break
            fi
            sleep 2
          done
          grep -n "Task .* received" "${EVIDENCE_DIR}/worker.log" | tail -n 10 > "${EVIDENCE_DIR}/worker_received.txt" || true
          if [ ! -s "${EVIDENCE_DIR}/worker_received.txt" ]; then
            grep -n "matview_pulse_task_start" "${EVIDENCE_DIR}/worker.log" | tail -n 10 > "${EVIDENCE_DIR}/worker_received.txt" || true
          fi
          if [ ! -s "${EVIDENCE_DIR}/worker_received.txt" ]; then
            tail -n 200 "${EVIDENCE_DIR}/worker.log" > "${EVIDENCE_DIR}/worker_received.txt" || true
          fi
          grep -n "pulse_matviews_global" "${EVIDENCE_DIR}/worker.log" | tail -n 10 > "${EVIDENCE_DIR}/worker_pulse_received.txt" || true
          grep -n "matview_pulse_task_start" "${EVIDENCE_DIR}/worker.log" | tail -n 5 > "${EVIDENCE_DIR}/worker_pulse_start.txt" || true
          grep -n "matview_pulse_task_dispatched" "${EVIDENCE_DIR}/worker.log" | tail -n 5 > "${EVIDENCE_DIR}/worker_pulse_dispatched.txt" || true
          grep -n "celery_worker_registered_tasks" "${EVIDENCE_DIR}/worker.log" | tail -n 5 > "${EVIDENCE_DIR}/worker_registered_tasks.txt" || true

          CORRELATION_ID="22222222-2222-2222-2222-222222222222"
          KWARGS=$(python - <<'PY'
          import json
          print(json.dumps({
              "tenant_id": "11111111-1111-1111-1111-111111111111",
              "view_name": "mv_nonexistent",
              "correlation_id": "22222222-2222-2222-2222-222222222222",
              "schedule_class": "minute",
          }))
          PY
          )
          celery -A app.celery_app.celery_app call app.tasks.matviews.refresh_single \
            --kwargs "${KWARGS}" 2>&1 | tee "${EVIDENCE_DIR}/force_failure_call.txt" | tail -n 1 || true

          FORCE_TASK_ID=""
          FORCE_TASK_ID="$(grep -Eo "[0-9a-fA-F-]{36}" "${EVIDENCE_DIR}/force_failure_call.txt" | tail -n 1 || true)"

          sleep 5
          psql "${DATABASE_URL}" -c "select count(*) from worker_failed_jobs;" > "${EVIDENCE_DIR}/worker_failed_jobs_after.txt"
          if [ -n "${FORCE_TASK_ID}" ]; then
            psql "${DATABASE_URL}" -c "select task_name, correlation_id, error_type, error_message from worker_failed_jobs where task_id='${FORCE_TASK_ID}' order by failed_at desc limit 5;" > "${EVIDENCE_DIR}/worker_failed_jobs_row.txt"
          else
            psql "${DATABASE_URL}" -c "select task_name, correlation_id, error_type, error_message from worker_failed_jobs order by failed_at desc limit 5;" > "${EVIDENCE_DIR}/worker_failed_jobs_row.txt"
          fi
          psql "${DATABASE_URL}" -c "select count(*) from kombu_message;" > "${EVIDENCE_DIR}/kombu_message_after.txt"

          if [ -f "${EVIDENCE_DIR}/beat.pid" ]; then
            kill "$(cat "${EVIDENCE_DIR}/beat.pid")" || true
          fi
          if [ -f "${EVIDENCE_DIR}/worker.pid" ]; then
            kill "$(cat "${EVIDENCE_DIR}/worker.pid")" || true
          fi

          ls -l "${EVIDENCE_DIR}" > "${EVIDENCE_DIR}/evidence_manifest.txt"

      - name: Assert evidence non-empty
        run: |
          set -euo pipefail
          EVIDENCE_DIR="${GITHUB_WORKSPACE}/evidence/b0545_ci"
          required=(
            "beat.log"
            "worker.log"
            "inspect_ping.txt"
            "inspect_registered.txt"
            "inspect_active_queues.txt"
            "inspect_stats.txt"
            "celery_config.json"
            "beat_due_task.txt"
            "beat_pulse_task.txt"
            "worker_received.txt"
            "worker_pulse_received.txt"
            "worker_registered_tasks.txt"
            "worker_failed_jobs_after.txt"
            "worker_failed_jobs_row.txt"
            "evidence_manifest.txt"
          )
          for name in "${required[@]}"; do
            path="${EVIDENCE_DIR}/${name}"
            if [ ! -s "${path}" ]; then
              echo "Missing or empty evidence file: ${path}"
              exit 1
            fi
          done
          if ! grep -E -q "(pong|OK)" "${EVIDENCE_DIR}/inspect_ping.txt"; then
            echo "No celery pong response captured"
            exit 1
          fi
          if ! grep -q "pulse_matviews_global" "${EVIDENCE_DIR}/beat_pulse_task.txt"; then
            echo "Beat did not emit pulse_matviews_global in evidence"
            exit 1
          fi
          if ! grep -q "pulse_matviews_global" "${EVIDENCE_DIR}/worker_pulse_received.txt"; then
            echo "Worker did not receive pulse_matviews_global in evidence"
            exit 1
          fi
          if ! grep -q "app.tasks.matviews.refresh_single" "${EVIDENCE_DIR}/worker_failed_jobs_row.txt"; then
            echo "Forced failure did not persist DLQ evidence"
            exit 1
          fi

      - name: Upload B0545 kinetic evidence artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: b0545-kinetic-evidence-${{ github.sha }}
          path: evidence/b0545_ci/
          retention-days: 30
